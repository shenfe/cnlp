# adagrad

[SGD](../sgd)、[Momentum](../momentum)、[Nesterov Momentum](../nesterov_momentum)等优化方法对于所有参数都使用了同一个更新速率。但是同一个更新速率不一定适合所有参数。比如有的参数可能已经到了仅需要微调的阶段，但又有些参数由于对应样本少等原因，还需要较大幅度的调动。

Adagrad就是针对这一问题提出的，自适应地为各个参数分配不同学习率的算法。

其公式如下：

$$\Delta x_{t}= - \frac{\eta}{\sqrt{\sum_{\tau=1}^t{g_{\tau}^2}+\epsilon}}g_t$$

其中 $g_t$ 同样是当前的梯度，连加和开根号都是元素级别的运算。$\eta$ 是初始学习率，由于之后会自动调整学习率，所以初始值就不像之前的算法那样重要了。而 $\epsilon$ 是一个比较小的数，用来保证分母非0。

其含义是，对于每个参数，随着其更新的总距离增多，其学习速率也随之变慢。

Adagrad算法存在三个问题：

1. 其学习率是单调递减的，训练后期学习率非常小
1. 其需要手工设置一个全局的初始学习率
1. 更新 $x_t$ 时，左右两边的单位不同一

[Adadelta](../adadelta)针对上述三个问题提出了比较漂亮的解决方案。