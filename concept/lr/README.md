# lr

> Logistic Regression, 逻辑回归

Logistic回归主要用于二分类问题, 利用Logistic函数(Sigmoid函数), 自变量取值范围为(-INF, INF), 自变量的取值范围为(0, 1).

## 逻辑回归 vs 线性回归

逻辑回归的模型本质上也是一个线性回归模型.

经典线性模型的优化目标函数是最小二乘, 而逻辑回归则是似然函数.

线性回归在整个实数域范围内进行预测, 敏感度一致, 无法做到sigmoid的非线性形式, 而逻辑回归使用sigmoid函数, 将预测值限定为[0, 1]间, 可以轻松处理0/1分类问题.

## 最小二乘法 vs 最大似然估计

对于最小二乘法, 当从模型总体随机抽取n组样本观测值后, 最合理的参数估计量应该使得模型能最好地拟合样本数据, 也就是估计值和观测值之差的平方和最小.

对于最大似然法, 当从模型总体随机抽取n组样本观测值后, 最合理的参数估计量应该使得从模型中抽取该n组样本观测值的概率最大.

在最大似然法中, 通过选择参数, 使已知数据在某种意义下最有可能出现, 而某种意义通常指似然函数最大, 而似然函数又往往指数据的概率分布函数. 与最小二乘法不同的是, 最大似然法需要已知这个概率分布函数, 这在实践中是很困难的. 一般假设其满足正态分布函数的特性, 在这种情况下, 最大似然估计和最小二乘估计相同.

最小二乘法以估计值与观测值的差的平方和作为损失函数, 极大似然法则是以最大化目标值的似然概率函数为目标函数, 从概率统计的角度处理线性回归并在似然概率函数为高斯函数的假设下同最小二乘建立了的联系.

最小二乘: 找到一个(组)估计值, 使得实际值与估计值的距离最小. 本来用两者差的绝对值汇总并使之最小是最理想的, 但绝对值在数学上求最小值比较麻烦, 因而替代做法是, 找一个(组)估计值, 使得实际值与估计值之差的平方加总之后的值最小, 称为最小二乘. "二乘"的英文为leastsquare, 其实英文的字面意思是"平方最小". 这时, 将这个差的平方的和式对参数求导数, 并取一阶导数为零, 就是OLSE.

最大似然估计: 现在已经拿到了很多个样本(你的数据集中所有因变量), 这些样本值已经实现, 最大似然估计就是去找到那个(组)参数估计值, 使得前面已经实现的样本值发生概率最大. 因为你手头上的样本已经实现了, 其发生概率最大才符合逻辑. 这时是求样本所有观测的联合概率最大化, 是个连乘积, 只要取对数, 就变成了线性加总. 此时通过对参数求导数, 并令一阶导数为零, 就可以通过解方程(组), 得到最大似然估计值.

## 解法

1. 梯度下降法
    1. 最大似然估计得到cost function
    1. 用梯度上升或者牛顿法求解参数
1. 牛顿法

### 梯度下降法 vs 牛顿法

梯度下降法和牛顿法都是为了求函数最优解, 但是方式不同.

梯度下降法的步骤是, 选择一组随机值, 计算函数的导数, 接着沿着导数的反方向, 也就是沿着下降的方向前进一步, 逐步逼近最小值.

牛顿法的核心思想是, 而当一个函数存在极值时, 它的极值点处的一阶导数等于0, 那么用迭代的方法逐步逼近一阶导数为0的位置.

和梯度下降相比, 牛顿方法的收敛速度更快, 通常只要十几次或者更少就可以收敛, 牛顿方法也被称为二次收敛(quadratic convergence), 因为当迭代到距离收敛值比较近的时候, 每次迭代都能使误差变为原来的平方. 缺点是当参数向量较大的时候, 每次迭代都需要计算一次 Hessian 矩阵的逆, 比较耗时.

## 优化

随机梯度法

